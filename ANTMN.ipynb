{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb941448e966a02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ANTMN with BERTopic\n",
    "The following is an implementation of Walter & Ophir's (2019) __Analysis of Topic Model Networks__ method using the BERTopic topic modelling architecture on a Hungarian news corpus. Unfortunately none of the textual data used for this project can be legally published, therefore the following is merely a methodological demonstration. \n",
    "<br>\n",
    "\n",
    "For the original method see: Walter, D. and Ophir, Y., 2019. News frame analysis: An inductive mixed-method computational approach. Communication Methods and Measures, 13(4), pp.248-266.\n",
    "<br>\n",
    "For the BERTopic implementation below see: Nagy, M., 2024. Testing an Inductive Mixed-method Computational Approach to News Frame Analysis: An analysis of Hungarian online reporting of the 2014 Russia-Ukraine conflict. https://repository.ifla.org/handle/123456789/3402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604915f5-89e4-4c84-893f-e803db2f8eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T08:45:49.772261Z",
     "start_time": "2024-07-29T08:45:49.762244Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import spacy\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "import igraph as ig\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib import dates as mdates\n",
    "%matplotlib inline\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666d416e-00b6-4f6f-9fe1-b6d3f409f24a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T08:45:52.320933Z",
     "start_time": "2024-07-29T08:45:52.315954Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73466d5-f1a5-4f83-b6b3-f6ba41bb526c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cde8c23-af4d-4a25-856a-95b1437baf8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T08:45:59.173934Z",
     "start_time": "2024-07-29T08:45:58.944668Z"
    }
   },
   "outputs": [],
   "source": [
    "np_mean = partial(np.mean, axis=0)  # Matrix mean calculation\n",
    "\n",
    "def _np_max(stuff):\n",
    "    \"\"\"Matrix max calculation\"\"\"\n",
    "    return np.max(list(stuff), axis=0)\n",
    "\n",
    "def normalize_node_size(data):\n",
    "    \"\"\"Network node size normalizer\"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def sil_and_calinski(topic_model, topics, probs, embeddings, name=None):\n",
    "    \"\"\"Calculate Shiluette (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) \n",
    "      and Calinski and Harabasz (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html) \n",
    "      scores for checking cluster quality\n",
    "      \n",
    "      Calinski-Harabasz Index is only for information, not as useful in DBSCAN based clustering. \n",
    "      \"\"\"\n",
    "    \n",
    "    # Remove outlier topics\n",
    "    # https://github.com/MaartenGr/BERTopic/issues/428#issuecomment-1027647827\n",
    "    umap_embeddings = topic_model.umap_model.transform(embeddings)\n",
    "    indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
    "    X = umap_embeddings[np.array(indices)]\n",
    "    labels = [topic for index, topic in enumerate(topics) if topic != -1]\n",
    "    s = silhouette_score(X, labels)\n",
    "    \n",
    "    c = calinski_harabasz_score(embeddings, topics)\n",
    "\n",
    "    print(f'--> silhouette score: {s} / calinski_harabasz score: {c}')\n",
    "\n",
    "def test_sil_cal(embeddings, texts, name=None, ranges=(10, 30)):\n",
    "    \"\"\"Iteratively test topic clustering with Shiluette and Calinski-Harabasz Index scores \n",
    "        with range of minimum cluster size \n",
    "        (https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#min_cluster_size)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    embeddings = np.array(list(embeddings))\n",
    "    # Test mean embeddings\n",
    "    if name is not None:\n",
    "        print(f'Testing {name}')\n",
    "        \n",
    "    for i in range(*ranges, 5):\n",
    "        print(f'min_cluster_size {i}', end=' ')\n",
    "        topic_model = get_model(i)\n",
    "        topics, probs = topic_model.fit_transform(texts, embeddings=embeddings)\n",
    "        sil_and_calinski(topic_model, topics, probs, embeddings=embeddings, name=name)\n",
    "        \n",
    "\n",
    "def get_model(n, auto_reduce_topics=False, spacy_rep=False, mmr=False):\n",
    "    \"\"\"Main function for creating BERTopic model.\n",
    "    :params: n: HDBSCAN min_cluster_size\"\"\"\n",
    "    # Embeddings\n",
    "    embeddings_model = SentenceTransformer('NYTK/sentence-transformers-experimental-hubert-hungarian')\n",
    "    # Preventing Stochastic Behaviour\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    # Controlling Number of Topics\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=n, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    # Improving Default Representation\n",
    "    with open('hu-stopwords.txt') as fh:\n",
    "        hu_stopwords = [l.strip() for l in fh.readlines()]\n",
    "    vectorizer_model = CountVectorizer(stop_words=hu_stopwords, min_df=2, ngram_range=(1, 3))\n",
    "    # Frequent words remover\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "    # Additional Representations\n",
    "    representation_model = {}\n",
    "    representation_model['KeyBERT'] = KeyBERTInspired()\n",
    "    \n",
    "    if spacy_rep:   \n",
    "        spacy_model = spacy.load('hu_core_news_lg')\n",
    "        # # Part-of-Speech\n",
    "        representation_model['POS'] = PartOfSpeech(spacy_model)\n",
    "    # MMR\n",
    "    if mmr:\n",
    "        mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "        representation_model['MMR'] = mmr_model\n",
    "\n",
    "    bertopic_args = {\n",
    "        # Pipeline models\n",
    "        'embedding_model': embeddings_model,\n",
    "        'umap_model': umap_model,\n",
    "        'hdbscan_model': hdbscan_model,\n",
    "        'vectorizer_model': vectorizer_model,\n",
    "        'ctfidf_model': ctfidf_model,\n",
    "        'representation_model': representation_model,\n",
    "        # Hyperparameters\n",
    "        'top_n_words': 10,\n",
    "        'verbose': True,\n",
    "        # Probs\n",
    "        'calculate_probabilities': True\n",
    "        }\n",
    "\n",
    "    if auto_reduce_topics:\n",
    "        bertopic_args['nr_topics'] = 'auto'\n",
    "        \n",
    "    # Training\n",
    "    tm = BERTopic(**bertopic_args)\n",
    "    print('Delivering model')\n",
    "    return tm\n",
    "\n",
    "\n",
    "def draw_network(Graph, cluster_name, scale=1, seed=None, k=None, iterations=50,  \n",
    "                 save=None, alpha=0.25, font_size=5, with_labels=True):\n",
    "    \"\"\"Main function for drawing network from networkX Graph object.\n",
    "    :params: Graph: NetworkX Graph object with calculated attributes that include cluster_name.\n",
    "    :params: cluster_name: Chosen attribute for coloring nodes\n",
    "    :params: scale: scale argument for spring layout\n",
    "    :params: seed: Seed number for spring layout\n",
    "    :params: k: Spring layout optimal distance between nodes\n",
    "    :params: iterations: Spring layout maximum number of iterations taken\n",
    "    :params: save: str or None, if str, graph png is saved with str as name\n",
    "    :params: alpha: Opacity of network edges\n",
    "    :params: font_size: Font size of node labels\n",
    "    :params: with_labels: Bool, if False node labels are omitted\"\"\"\n",
    "    \n",
    "    Graph = nx.relabel_nodes(Graph, nx.get_node_attributes(Graph, \"topic_label\"))\n",
    "    \n",
    "    # Spring layout positions for all nodes - seed for reproducibility\n",
    "    pos = nx.spring_layout(Graph, seed=seed, scale=scale, k=k, iterations=iterations)  \n",
    "        \n",
    "    node_sizes = list(nx.get_node_attributes(Graph, 'size').values())\n",
    "    node_colors = list(nx.get_node_attributes(Graph, cluster_name).values())\n",
    "    nx.draw_networkx_nodes(Graph, pos, node_color=node_colors, node_size=node_sizes)\n",
    "    \n",
    "    nx.draw_networkx_edges(\n",
    "        Graph, pos, \n",
    "        edgelist=Graph.edges(),\n",
    "        width=[e[2]['weight'] for e in Graph.edges(data=True)],\n",
    "        alpha=alpha, \n",
    "        edge_color=\"b\", \n",
    "        style=\"dashed\")\n",
    "    \n",
    "    # node labels\n",
    "    if with_labels:\n",
    "        nx.draw_networkx_labels(Graph, pos, labels=nx.get_node_attributes(Graph, \"topic_label\"), font_size=font_size, font_family=\"sans-serif\")\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.margins(0.1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create a list to hold the text objects for the labels\n",
    "    texts = []\n",
    "    \n",
    "    # Draw labels and store the text objects\n",
    "    for node, (x, y) in pos.items():\n",
    "        texts.append(plt.text(x, y, node, fontsize=font_size, ha='center', va='center'))\n",
    "    \n",
    "    # Adjust the text labels to prevent overlap\n",
    "    adjust_text(texts, only_move={'points': 'xy', 'texts': 'xy'}, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n",
    "\n",
    "    if save is not None:\n",
    "        plt.savefig(f'output_graphs/{save}', dpi=200)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# The following are community detection functions that return a tuple of a ID:color dict and a \n",
    "#  list of sets which contain IDs for a given community \n",
    "\n",
    "def get_louvain(nxGraph):\n",
    "    \"\"\"https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html\"\"\"\n",
    "    louvain = nx.community.louvain_communities(nxGraph, weight='weight', resolution=1)\n",
    "    louvain_coms = {}\n",
    "    for c_num, com in enumerate(louvain):\n",
    "        for top in com:\n",
    "            louvain_coms[top] = c_num\n",
    "    \n",
    "    colors = ['lightblue', 'red', 'orange', 'gray', 'orange', 'pink', 'yellow', 'lightgreen']\n",
    "    louvain_colors = {}\n",
    "    for top, community in louvain_coms.items():\n",
    "        louvain_colors[top] = colors[community]\n",
    "    return louvain_colors, louvain\n",
    "\n",
    "def get_walktrap(nxGraph):\n",
    "    \"\"\"https://igraph.org/python/doc/api/igraph._igraph.GraphBase.html#community_walktrap\"\"\"\n",
    "    # convert to igraph\n",
    "    iG = ig.Graph.from_networkx(nxGraph)\n",
    "    \n",
    "    walktrap = iG.community_walktrap(weights='weight')\n",
    "    communities = walktrap.as_clustering()\n",
    "    \n",
    "    walktrap_coms = {}\n",
    "    for c_num, com in enumerate(communities):\n",
    "        for top in com:\n",
    "            walktrap_coms[top] = c_num\n",
    "    \n",
    "    colors = ['lightblue', 'red', 'orange', 'gray', 'orange', 'pink', 'yellow', 'lightgreen']\n",
    "    walktrap_colors = {}\n",
    "    for top, community in walktrap_coms.items():\n",
    "        walktrap_colors[top] = colors[community]\n",
    "\n",
    "    return walktrap_colors, list(communities)\n",
    "    \n",
    "def get_spinglass(nxGraph):\n",
    "    \"\"\"https://igraph.org/python/doc/api/igraph._igraph.GraphBase.html#community_spinglass\"\"\"\n",
    "    # convert to igraph\n",
    "    iG = ig.Graph.from_networkx(nxGraph)\n",
    "    spinglass = iG.community_spinglass(weights='weight')\n",
    "    spinglass_coms = dict(zip([i['_nx_name'] for i in iG.vs], spinglass.membership))\n",
    "    \n",
    "    colors = ['lightblue', 'red', 'orange', 'gray', 'orange', 'pink', 'yellow', 'lightgreen', 'darkgreen', 'grey', 'darkblue'] + \\\n",
    "         ['lightblue', 'red', 'orange', 'gray', 'orange', 'pink', 'yellow', 'lightgreen', 'darkgreen', 'grey', 'darkblue']\n",
    "    spinglass_colors = {}\n",
    "    spinglass_community_dict = defaultdict(list)\n",
    "    for top, community in spinglass_coms.items():\n",
    "        spinglass_colors[top] = colors[community]\n",
    "        spinglass_community_dict[community].append(top)\n",
    "        \n",
    "    return spinglass_colors, [[i['_nx_name'] for i in g.vs] for g in spinglass.subgraphs()]\n",
    "    \n",
    "def get_fast_greedy(nxGraph):\n",
    "    \"\"\"https://igraph.org/python/doc/api/igraph._igraph.GraphBase.html#community_fastgreedy\"\"\"\n",
    "    iG = ig.Graph.from_networkx(nxGraph)\n",
    "\n",
    "    fast_greedy = iG.community_fastgreedy(weights='weight')\n",
    "    fast_greedy_communities = fast_greedy.as_clustering()\n",
    "    \n",
    "    fast_greedy_coms = {}\n",
    "    for c_num, com in enumerate(fast_greedy_communities):\n",
    "        for top in com:\n",
    "            fast_greedy_coms[top] = c_num\n",
    "    \n",
    "    colors = ['lightblue', 'red', 'orange', 'gray', 'orange', 'pink', 'yellow', 'lightgreen']\n",
    "    fast_greedy_colors = {}\n",
    "    for top, community in fast_greedy_coms.items():\n",
    "        fast_greedy_colors[top] = colors[community]\n",
    "\n",
    "    return fast_greedy_colors, list(fast_greedy_communities)\n",
    "\n",
    "def get_eigenvector(nxGraph):\n",
    "    \"\"\"https://igraph.org/python/doc/api/igraph._igraph.GraphBase.html#community_leading_eigenvector\"\"\"\n",
    "    iG = ig.Graph.from_networkx(nxGraph)\n",
    "    leading_eigenvector = iG.community_leading_eigenvector(weights='weight')\n",
    "    leading_eigenvector_communities = dict(zip([i['_nx_name'] for i in iG.vs], leading_eigenvector.membership))\n",
    "    \n",
    "    colors = ['lightblue', 'red', 'orange', 'gray', 'orange', 'pink', 'yellow', 'lightgreen']\n",
    "    leading_eigenvector_colors = {}\n",
    "    for top, community in leading_eigenvector_communities.items():\n",
    "        leading_eigenvector_colors[top] = colors[community]\n",
    "    return leading_eigenvector_colors, [[i['_nx_name'] for i in g.vs] for g in leading_eigenvector.subgraphs()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea8f3824fbba6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import data\n",
    "Minimum data requirements: the news data must contain a column for identifying the article (e.g. unique identifier ID) and a column for the text itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c7279c3e9c5bd090",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T13:46:02.507399Z",
     "start_time": "2024-07-26T13:46:01.804388Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/path/to/your/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31880d6c103eac0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Calculate embeddings\n",
    "In the following section two methods are introduced for calculating embeddings: \n",
    "1) Whole text embedding (where either the first n tokens of the text are considered accoring to the sentence transformer model, or shorter texts can be used such as lead paragraphs)\n",
    "2) Sentence based embeddings (where later averaging of embeddings is required)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13bacb841cb2143",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embedding model\n",
    "For Hungarian Experimental Sentence-BERT model created by the Hungarian Research Centre for Linguistics\n",
    "https://huggingface.co/NYTK/sentence-transformers-experimental-hubert-hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "562831e1bab88ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T09:51:39.147740Z",
     "start_time": "2024-07-23T09:51:16.592329Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_transformer_model = SentenceTransformer('NYTK/sentence-transformers-experimental-hubert-hungarian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565daa715ec90e6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Whole text embedding\n",
    "As the SentenceTransformers embedding library was created for short texts, only lead paragraphs are advised as whole texts, or even just titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "31354c52ecc9ba05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T09:50:49.643378Z",
     "start_time": "2024-07-23T09:50:49.637685Z"
    }
   },
   "outputs": [],
   "source": [
    "whole_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44999e1b-a8f3-4de0-af6f-3706a1411271",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_text_embeddings = sentence_transformer_model.encode(whole_df['text'].to_list(), show_progress_bar=True)  # Use whatever the text column name is for 'text'\n",
    "whole_df['embedding'] = list(whole_text_embeddings)  # Append embeddings column to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa71fc56f8642",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Sentence based embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06d0b33b20010d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T10:17:10.387505Z",
     "start_time": "2024-07-23T10:16:39.490996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download spacy model for sentence splitting\n",
    "# Use whatever model here, I use hu_core_new_lg https://huggingface.co/huspacy/hu_core_news_lg\n",
    "!pip install https://huggingface.co/huspacy/hu_core_news_lg/resolve/main/hu_core_news_lg-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bbd607cac57a308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T10:17:26.056956Z",
     "start_time": "2024-07-23T10:17:23.425402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x71042e376dd0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize spacy model for sentence splitting\n",
    "nlp = spacy.load('hu_core_news_lg')  \n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270748b-153d-4af8-90e7-67282646db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ids = []\n",
    "sentence_texts = []\n",
    "\n",
    "\n",
    "# Iterate through each text and split texts into sentences\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    identifier = row['id']  # Get identifier column value\n",
    "    text = row['text']  # Get text column value\n",
    "    \n",
    "    c = 0  # Counter for number of sentences\n",
    "\n",
    "    text_proc = nlp(text)\n",
    "\n",
    "    for sentence in text_proc.sents:\n",
    "        c += 1\n",
    "        sentence_ids.append((identifier, c))\n",
    "        sentence_texts.append(sentence.text)\n",
    "\n",
    "# Calculate sentence embeddings    \n",
    "sentence_embeddings = list(sentence_transformer_model.encode(sentence_texts, show_progress_bar=True))\n",
    "# Create multi-index of file and paragraph number\n",
    "sentence_embeddings_index = pd.MultiIndex.from_tuples(sentence_ids, names=['id', 'par_num'])\n",
    "# Create dataframe where multi-index is file and paragraph number, and columns are sentences and sentence embeddings\n",
    "sent_df = pd.DataFrame({'text': sentence_texts, 'embedding': sentence_embeddings}, index=sentence_embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7729159-ddad-4fc7-b0a4-891ed9a3898e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get topics\n",
    "repeat steps for 1) whole text and 2) sentence based embeddings\n",
    "- Run Silhuette and Calinski test\n",
    "- Run get_model to create Topic model + use BERTopic visualizations to choose right HDBSCAN min_cluster_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c7b3d16d18e29",
   "metadata": {},
   "source": [
    "## 1) Whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9885e8666ed6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run min_cluster_size tests\n",
    "# --> High silhuette score is good, consider higher Calinski-Harabasz Index, however, rely more on silhuette\n",
    "test_sil_cal(whole_df['embedding'], whole_df['text'], ranges=(10, 25))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c01528-0b9e-4b2b-b53e-7c80e930a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model for best number and see BERTopic topic model distribution visualization. I usually chose the min_cluster_size number where the visualization seemed most convincing\n",
    "min_cluster_size = 15 # change this around to see, 15 usually works best\n",
    "model = get_model(min_cluster_size)\n",
    "whole_topics, whole_probs = model.fit_transform(whole_df['text'], embeddings=np.array(list(whole_df['embedding'])))\n",
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2cb0608a64677",
   "metadata": {},
   "source": [
    "## 2) Sentence based embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1aebe5-4d70-480f-9639-e9d2bf174974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run min_cluster_size tests\n",
    "# --> High silhuette score is good, consider higher Calinski-Harabasz Index, however, rely more on silhuette\n",
    "test_sil_cal(sent_df['embedding'], sent_df['text'], ranges=(10, 25))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56abcda-fb67-4507-866c-0c4520d41a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best scoring model and see BERTopic topic model distribution visualization. I usually chose the min_cluster_size number where the visualization seemed most convincing\n",
    "min_cluster_size = 15 # change this around to see, 15 usually works best\n",
    "model = get_model(min_cluster_size)\n",
    "sent_topics, sent_probs = model.fit_transform(sent_df['text'], embeddings=np.array(list(sent_df['embedding'])))\n",
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca643ea159a30d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Annotate topics with topic names\n",
    "In this step you must create a topic_number:topic_title dictionary for the topic you generated. (BERTopic now has OpenAI integration so you can generate topic labels with GPT). Here you can filter or even join together topics if they seem either redundant or unintelligible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b13b7b29406c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_lables = {}  # 1: 'marriage', 2: 'children', 3: 'housing', etc...\n",
    "sent_labels = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3e21ffdf82f08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create networks based on pairwise probability scores\n",
    "Repeat for 1) whole text and 2) sentence based  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719fccbe6dcee51c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1) Whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "836b359b-3103-4183-bd55-a5e8dcb52895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create theta matrix\n",
    "whole_probs_df = pd.DataFrame(whole_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f9fc9ac7-6ebd-4a36-a240-cd10c7fb2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create theta matrix\n",
    "wG = nx.from_numpy_array(cosine_similarity(pd.DataFrame(whole_probs_df).transpose()), edge_attr='weight')\n",
    "node_sizes = {k: v for k, v in zip(whole_probs_df.columns, [i*600 for i in normalize_node_size(whole_probs_df.sum())])}\n",
    "nx.set_node_attributes(wG, node_sizes, name='size')\n",
    "wG.remove_edges_from(nx.selfloop_edges(wG))\n",
    "nx.set_node_attributes(wG, whole_lables, name='topic_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8aed7656-0d05-4568-b83b-c7be0c8606fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate clusters (second from tuple only for analysis)\n",
    "w_louvain_colors, w_louvain = get_louvain(wG)\n",
    "w_walktrap_colors, w_walktrap = get_walktrap(wG)\n",
    "w_eigenvector_colors, w_eigenvector = get_eigenvector(wG)\n",
    "w_fast_greedy_colors, w_fast_greedy = get_fast_greedy(wG)\n",
    "w_spinglass_colors, w_spinglass = get_spinglass(wG)\n",
    "\n",
    "nx.set_node_attributes(wG, w_louvain_colors, name='louvain')\n",
    "nx.set_node_attributes(wG, w_walktrap_colors, name='walktrap')\n",
    "nx.set_node_attributes(wG, w_eigenvector_colors, name='eigenvector')\n",
    "nx.set_node_attributes(wG, w_fast_greedy_colors, name='fast_greedy')\n",
    "nx.set_node_attributes(wG, w_spinglass_colors, name='spinglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1974a-aebc-4928-9fa0-e923c8d28a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_network(wG, 'louvain')  # Change 'louvain' for other names of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af2b32765a175d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2) Sentence based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43eeccf3-8d8d-4cfe-a5bd-45454f689607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create theta matrix\n",
    "sent_topic_df = sent_df.copy()\n",
    "sent_topic_df['probs'] = list(sent_probs)\n",
    "# Here exchange _np_max for _np_mean if you want to mean the sentence probabilities instead of taking the max probability\n",
    "sent_topic_df_gp = sent_topic_df.groupby(sent_topic_df.index.get_level_values(0)).agg({'probs': _np_max, 'text': ' '.join})\n",
    "sent_probs_df = pd.DataFrame(sent_topic_df_gp['probs'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3e08b44f-9b10-4820-987f-fb8d4f81bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create newtork\n",
    "sG = nx.from_numpy_array(cosine_similarity(pd.DataFrame(sent_probs_df).transpose()), edge_attr='weight')\n",
    "size_cons = 300\n",
    "node_sizes = {k: v for k, v in zip(sent_probs_df.columns, [i*size_cons for i in normalize_node_size(sent_probs_df.sum())])}\n",
    "nx.set_node_attributes(sG, node_sizes, name='size')\n",
    "sG.remove_edges_from(nx.selfloop_edges(sG))\n",
    "nx.set_node_attributes(sG, sent_labels, name='topic_label')\n",
    "sG.remove_nodes_from([n[0] for n in sG.nodes(data=True) if n[1]['topic_label'] is False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "64f78a02-cc14-4605-9b8c-36d2fdb00eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate clusters (second from tuple only for analysis)\n",
    "s_louvain_colors, s_louvain = get_louvain(sG)\n",
    "s_walktrap_colors, walktrap = get_walktrap(G)\n",
    "s_eigenvector_colors, s_eigenvector = get_eigenvector(sG)\n",
    "s_fast_greedy_colors, s_fast_greedy = get_fast_greedy(sG)\n",
    "s_spinglass_colors, s_spinglass = get_spinglass(sG)\n",
    "\n",
    "nx.set_node_attributes(sG, s_louvain_colors, name='louvain')\n",
    "nx.set_node_attributes(sG, s_walktrap_colors, name='walktrap')\n",
    "nx.set_node_attributes(sG, s_eigenvector_colors, name='eigenvector')\n",
    "nx.set_node_attributes(sG, s_fast_greedy_colors, name='fast_greedy')\n",
    "nx.set_node_attributes(sG, s_spinglass_colors, name='spinglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "83f70617-518e-4720-b807-73df71272b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_network(sG, 'louvain')  # Change 'louvain' for other names of clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
